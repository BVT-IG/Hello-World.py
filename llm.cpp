llama.cpp 是开源的，从 GitHub 下载源代码，然后编译（build）成可执行文件。步骤：打开命令提示符，创建工作文件夹：mkdir C:\llama-cpp
cd C:\llama-cpp（C:\ 是你的盘符，可改。）克隆（下载）仓库：git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp（如果 git 慢，用 VPN 或镜像：git clone https://gitee.com/mirrors/llama.cpp。）安装 Python 依赖（转换模型用）：pip install -r requirements.txt（如果报错，升级 pip：python -m pip install --upgrade pip。）编译（build）llama.cpp，支持 CUDA GPU：llama.cpp 用 CMake 构建。创建 build 文件夹：mkdir build
cd build生成构建文件（启用 CUDA）：cmake .. -DLLAMA_CUDA=ON -DCMAKE_BUILD_TYPE=Release（如果无 CUDA，省略 -DLLAMA_CUDA=ON，用 CPU。报错？检查 Visual Studio 和 CUDA 路径。）编译（可能需 5-10 分钟）：cmake --build . --config Release -j 8（-j 8 是并行编译，用你的 CPU 核心数替换 8。成功后，会生成 exe 文件如 llama-cli.exe、llama-server.exe。）验证安装：运行 ./bin/Release/llama-cli.exe --help（或直接 llama-cli --help 如果 PATH 加了）。看到帮助信息，就成功了！小白提示：编译失败常见原因：CUDA 路径不对（环境变量 CUDA_PATH 设为 C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.0）。参考 YouTube 视频“Complete Llama.cpp Build Guide 2025 (Windows + GPU)”搜索修复。3. 模型转换和下载（10 分钟）llama.cpp 用 GGUF 格式模型（高效压缩版）。你可以直接下载现成 GGUF，或从 Hugging Face（HF）转换 PyTorch 模型。选项 1：直接下载现成 GGUF 模型（推荐小白）去 Hugging Face 搜索“GGUF”模型，如 TheBloke/Llama-3-8B-Instruct-GGUF（中小模型，适合 P4）。下载 Q4_K_M.gguf 文件（~4-5GB，平衡大小/质量）到 C:\llama-cpp\models\ 文件夹。示例命令（llama.cpp 自带下载）：cd C:\llama-cpp
mkdir models
./llama.cpp/bin/Release/llama-cli.exe -hf TheBloke/Llama-3-8B-Instruct-GGUF -o models/llama-3-8b.gguf（-o 指定输出路径。首次跑会自动下载。）选项 2：从 HF 转换 PyTorch 模型到 GGUF安装转换工具（已在 requirements.txt）：pip install torch transformers huggingface_hub转换命令（示例：Llama 3 8B）：cd C:\llama-cpp
python convert_hf_to_gguf.py /path/to/hf-model --outfile models/llama-3-8b.gguf --outtype f16（/path/to/hf-model 是 HF 模型文件夹，如用 git clone https://huggingface.co/meta-llama/Llama-3-8B-Instruct 下载。--outtype f16 是 FP16 精度，GPU 友好。）